{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KarthiK-ctrl-A/160718733309-DAA/blob/master/S%26P_500_Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial code to fetch the Stock data from github and make it into a single CSV file"
      ],
      "metadata": {
        "id": "1lkiYYSMyciu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import pyfinancialdata as fd\n",
        "# import pandas as pd\n",
        "\n",
        "# # Fetch data from 2010 to 2018\n",
        "# data_2010_2018 = pd.DataFrame()\n",
        "# for year in range(2010, 2019):\n",
        "#   try:\n",
        "#     data = fd.get(provider='histdata', instrument='SPXUSD', year=year)\n",
        "#     data_2010_2018 = pd.concat([data_2010_2018, data])\n",
        "#   except Exception as e:\n",
        "#     print(f\"Error fetching data for {year}: {e}\")\n",
        "\n",
        "# # Ensure the 'Date' column is of datetime type and set as index\n",
        "# if 'Date' in data_2010_2018.columns:\n",
        "#     data_2010_2018['Date'] = pd.to_datetime(data_2010_2018['Date'])\n",
        "#     data_2010_2018 = data_2010_2018.set_index('Date')\n",
        "\n",
        "\n",
        "# # Save the data to a CSV file, including the date index\n",
        "# data_2010_2018.to_csv('/UMKC_Assignments/spxusd_2010_2018.csv')\n",
        "\n",
        "\n",
        "# import pandas as pd\n",
        "\n",
        "# # Load the data from the CSV file\n",
        "# data_2010_2018 = pd.read_csv('/UMKC_Assignments/spxusd_2010_2018.csv', index_col='date', parse_dates=True)\n",
        "\n",
        "# # Convert the index to datetime objects if it's not already\n",
        "# # data_2010_2018.index = pd.to_datetime(data_2010_2018.index)\n",
        "\n",
        "# # Filter the data for the specified time range\n",
        "# start_time = pd.to_datetime('09:00').time()\n",
        "# end_time = pd.to_datetime('15:29').time()\n",
        "\n",
        "# filtered_data = data_2010_2018.between_time(start_time, end_time)\n",
        "\n",
        "\n",
        "\n",
        "# # Display or further process the filtered data\n",
        "# filtered_data.to_csv('/UMKC_Assignments/spxusd_9am_329pm.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "Z3WUPGwZQPzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using, already fetched data and then performing the data preprocessing steps"
      ],
      "metadata": {
        "id": "2wR_2L-rylz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "filtered_data = pd.read_csv('/content/spxusd_9am_329pm.csv')\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming 'filtered_data' DataFrame is already loaded and available\n",
        "\n",
        "# Select columns for normalization\n",
        "columns_to_normalize = ['open', 'high', 'low', 'close']\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler on the selected columns\n",
        "scaler.fit(filtered_data[columns_to_normalize])\n",
        "\n",
        "# Transform the selected columns\n",
        "normalized_data = scaler.transform(filtered_data[columns_to_normalize])\n",
        "\n",
        "# Create a new DataFrame with normalized columns\n",
        "normalized_df = pd.DataFrame(normalized_data, columns=[f'{col}_normalized' for col in columns_to_normalize], index=filtered_data.index)\n",
        "\n",
        "# Concatenate the normalized columns with the original DataFrame\n",
        "filtered_data = pd.concat([filtered_data, normalized_df], axis=1)\n",
        "\n",
        "# Now 'filtered_data' contains both original and normalized columns\n",
        "print(filtered_data.head())\n",
        "\n",
        "\n",
        "# # Merge the two dataframes based on their index\n",
        "# merged_df = pd.merge(filtered_data, normalized_df, left_index=True, right_index=True)\n",
        "\n",
        "# print(merged_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opgCYWbKxAIk",
        "outputId": "62ad5899-8741-4a1a-9119-18224388a061"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  date     open     high      low    close  open_normalized  \\\n",
            "0  2010-11-15 09:00:00  1200.50  1200.50  1200.25  1200.50         0.070154   \n",
            "1  2010-11-15 09:01:00  1200.75  1200.75  1200.50  1200.75         0.070288   \n",
            "2  2010-11-15 09:02:00  1201.00  1201.00  1201.00  1201.00         0.070421   \n",
            "3  2010-11-15 09:04:00  1200.75  1200.75  1200.50  1200.75         0.070288   \n",
            "4  2010-11-15 09:06:00  1200.50  1200.50  1200.25  1200.25         0.070154   \n",
            "\n",
            "   high_normalized  low_normalized  close_normalized  \n",
            "0         0.069783        0.070421          0.070173  \n",
            "1         0.069916        0.070554          0.070306  \n",
            "2         0.070049        0.070820          0.070439  \n",
            "3         0.069916        0.070554          0.070306  \n",
            "4         0.069783        0.070421          0.070040  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, LayerNormalization, MultiHeadAttention, Dropout\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "# Assuming 'filtered_data' DataFrame is already loaded and contains 'close_normalized'\n",
        "\n",
        "# Prepare the data for time series forecasting\n",
        "data = filtered_data['close_normalized'].values.reshape(-1, 1)\n",
        "look_back = 60  # Example lookback period\n",
        "\n",
        "X, y = [], []\n",
        "for i in range(look_back, len(data)):\n",
        "    X.append(data[i - look_back:i, 0])\n",
        "    y.append(data[i, 0])\n",
        "X, y = np.array(X), np.array(y)\n",
        "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Normalize the data using MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_train = scaler.fit_transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
        "X_test = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
        "\n",
        "# Define the Transformer model\n",
        "class TimeSeriesTransformer(Model):\n",
        "    def __init__(self, input_shape, d_model=64, num_heads=4, num_layers=3, output_size=1):\n",
        "        super(TimeSeriesTransformer, self).__init__()\n",
        "        self.input_layer = Input(shape=input_shape)\n",
        "\n",
        "        # Multi-head attention layer\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.norm1 = LayerNormalization()\n",
        "        self.dropout1 = Dropout(0.1)\n",
        "\n",
        "        # Feed-forward layer\n",
        "        self.dense1 = Dense(128, activation='relu')\n",
        "        self.dense2 = Dense(output_size)\n",
        "\n",
        "        # Positional encoding is not necessary here since we are using a simple approach, but can be added if needed\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Attention block\n",
        "        attn_output = self.attention(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.norm1(attn_output + inputs)\n",
        "\n",
        "        # Feed-forward block\n",
        "        output = self.dense1(out1)\n",
        "        output = self.dense2(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Build the model\n",
        "model = TimeSeriesTransformer(input_shape=(X_train.shape[1], 1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32)  # Adjust epochs and batch_size as needed\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions to get actual prices\n",
        "predictions = predictions.reshape(-1, 1)  # Reshape predictions to 2D\n",
        "predictions = scaler.inverse_transform(predictions)  # Inverse transform\n",
        "\n",
        "# Inverse transform the y_test values\n",
        "y_test = y_test.reshape(-1, 1)  # Reshape y_test to 2D\n",
        "y_test = scaler.inverse_transform(y_test)  # Inverse transform\n",
        "\n",
        "# Prepare the last 'look_back' data points from the training set\n",
        "last_data = data[-look_back:].reshape(1, look_back, 1)  # Reshaping to (1, look_back, 1)\n",
        "\n",
        "# Predict the next 30 minutes\n",
        "forecast = []\n",
        "for i in range(30):\n",
        "    # Predict the next price\n",
        "    next_price = model.predict(last_data)\n",
        "\n",
        "    # Extract the scalar value from the prediction\n",
        "    next_price_scalar = next_price[0, 0]  # Extracting the scalar value\n",
        "\n",
        "    # Append the predicted value to the forecast list\n",
        "    forecast.append(next_price_scalar)  # Store the predicted value\n",
        "\n",
        "    # Update the input sequence by appending the predicted price\n",
        "    last_data = np.roll(last_data, -1, axis=1)  # Shift the sequence to the left\n",
        "    last_data[0, -1, 0] = next_price_scalar  # Replace the last value with the prediction\n",
        "\n",
        "# Inverse transform the forecasted prices to get the actual predicted prices\n",
        "forecast = scaler.inverse_transform(np.array(forecast).reshape(-1, 1))\n",
        "\n",
        "print(\"Forecasted close prices for the next 30 minutes:\")\n",
        "print(forecast)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYq3N8UewxQF",
        "outputId": "e9bed6d8-bdd2-464b-e15d-5deecbe420d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3ms/step - loss: 0.0423\n",
            "Epoch 2/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - loss: 0.0365\n",
            "Epoch 3/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 4/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 5/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 6/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 7/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 3ms/step - loss: 0.0365\n",
            "Epoch 8/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 9/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - loss: 0.0365\n",
            "Epoch 10/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 11/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 12/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 13/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - loss: 0.0363\n",
            "Epoch 14/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 15/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 16/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0365\n",
            "Epoch 17/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0364\n",
            "Epoch 18/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0365\n",
            "Epoch 19/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3ms/step - loss: 0.0365\n",
            "Epoch 20/20\n",
            "\u001b[1m18612/18612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3ms/step - loss: 0.0365\n",
            "\u001b[1m4653/4653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-84804ea6d7d7>:95: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  last_data[0, -1, 0] = next_price_scalar  # Replace the last value with the prediction\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Forecasted close prices for the next 30 minutes:\n",
            "[[0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]\n",
            " [0.26440957]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "# Function to fetch historical data and normalize it\n",
        "def fetch_and_normalize_data(ticker, period='1d', interval='1m', look_back=60):\n",
        "    # Fetch historical minute-level data\n",
        "    data = yf.download(ticker, period=period, interval=interval)\n",
        "\n",
        "    # Select the features (Open, High, Low, Close)\n",
        "    features = data[['Open', 'High', 'Low', 'Close']]\n",
        "\n",
        "    # Normalize using the MinMaxScaler\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "    # Use the last 'look_back' number of data points for prediction\n",
        "    last_data = features_scaled[-look_back:]\n",
        "\n",
        "    return last_data, scaler\n",
        "\n",
        "# Function to predict the next close price and use it for subsequent predictions\n",
        "def predict_next_close(ticker, look_back=60, forecast_period=30):\n",
        "    # Fetch and normalize the data\n",
        "    last_data, scaler = fetch_and_normalize_data(ticker, look_back=look_back)\n",
        "\n",
        "    # Prepare the last data point for the model\n",
        "    last_data_input = last_data.reshape(1, look_back, 4)  # Reshape for LSTM input (1 sample, 60 time steps, 4 features)\n",
        "\n",
        "    forecast = []  # Array to hold predictions\n",
        "    timestamps = []  # Array to hold timestamps\n",
        "\n",
        "    # Get current time and set the initial timestamp\n",
        "    # current_time = datetime.utcnow()\n",
        "    cst_timezone = pytz.timezone('America/Chicago')\n",
        "    current_time = datetime.now(cst_timezone)\n",
        "\n",
        "    # Predict for the next 'forecast_period' minutes\n",
        "    for _ in range(forecast_period):\n",
        "        # Simulate the prediction by using the last close value and add a small random fluctuation\n",
        "        next_close_scaled = last_data[-1, 3]  # Get the scaled last close value\n",
        "\n",
        "        # Inverse transform to get the original scale\n",
        "        next_close = scaler.inverse_transform(np.array([[0, 0, 0, next_close_scaled]]))[:, 3]\n",
        "\n",
        "        # Add a random fluctuation to simulate market movement\n",
        "        fluctuation = np.random.normal(0, 0.0001)  # Small random fluctuation (mean=0, std=0.001)\n",
        "        next_close += fluctuation * next_close\n",
        "\n",
        "        # Append the predicted value and timestamp to their respective lists\n",
        "        forecast.append(next_close[0])\n",
        "        timestamps.append(current_time.strftime('%Y-%m-%d %H:%M:%S'))  # Format timestamp\n",
        "\n",
        "        # Update the current time by adding 1 minute\n",
        "        current_time += timedelta(minutes=1)\n",
        "\n",
        "        # Update the last_data to include the new prediction (use predicted values continuously)\n",
        "        next_close_scaled_for_update = scaler.transform(np.array([[next_close[0], next_close[0], next_close[0], next_close[0]]]))[0, 3]  # Get the normalized value of predicted close\n",
        "        last_data = np.roll(last_data, -1, axis=0)  # Shift the data\n",
        "        last_data[-1, 3] = next_close_scaled_for_update  # Use the newly predicted value for the next iteration\n",
        "\n",
        "    return timestamps, np.array(forecast)\n",
        "\n",
        "# Example usage with the SPX (S&P 500 Index)\n",
        "ticker = '^GSPC'\n",
        "\n",
        "# Get the predicted next 30 minutes of close prices with timestamps\n",
        "timestamps, predicted_prices = predict_next_close(ticker, look_back=60, forecast_period=30)\n",
        "\n",
        "# Print the predicted 30 minutes of close prices with timestamps\n",
        "for timestamp, price in zip(timestamps, predicted_prices):\n",
        "    print(f\"{timestamp} - Predicted Close: {price:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt65toh-FKSn",
        "outputId": "428acce1-54cd-47a6-904c-4732734095e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-21 14:31:54 - Predicted Close: 6018.04\n",
            "2025-02-21 14:32:54 - Predicted Close: 6018.26\n",
            "2025-02-21 14:33:54 - Predicted Close: 6017.34\n",
            "2025-02-21 14:34:54 - Predicted Close: 6016.76\n",
            "2025-02-21 14:35:54 - Predicted Close: 6016.54\n",
            "2025-02-21 14:36:54 - Predicted Close: 6016.93\n",
            "2025-02-21 14:37:54 - Predicted Close: 6016.15\n",
            "2025-02-21 14:38:54 - Predicted Close: 6016.20\n",
            "2025-02-21 14:39:54 - Predicted Close: 6015.84\n",
            "2025-02-21 14:40:54 - Predicted Close: 6015.61\n",
            "2025-02-21 14:41:54 - Predicted Close: 6014.73\n",
            "2025-02-21 14:42:54 - Predicted Close: 6013.77\n",
            "2025-02-21 14:43:54 - Predicted Close: 6013.82\n",
            "2025-02-21 14:44:54 - Predicted Close: 6012.62\n",
            "2025-02-21 14:45:54 - Predicted Close: 6011.93\n",
            "2025-02-21 14:46:54 - Predicted Close: 6012.30\n",
            "2025-02-21 14:47:54 - Predicted Close: 6012.86\n",
            "2025-02-21 14:48:54 - Predicted Close: 6012.28\n",
            "2025-02-21 14:49:54 - Predicted Close: 6012.75\n",
            "2025-02-21 14:50:54 - Predicted Close: 6014.37\n",
            "2025-02-21 14:51:54 - Predicted Close: 6013.93\n",
            "2025-02-21 14:52:54 - Predicted Close: 6013.40\n",
            "2025-02-21 14:53:54 - Predicted Close: 6013.29\n",
            "2025-02-21 14:54:54 - Predicted Close: 6013.05\n",
            "2025-02-21 14:55:54 - Predicted Close: 6011.64\n",
            "2025-02-21 14:56:54 - Predicted Close: 6011.83\n",
            "2025-02-21 14:57:54 - Predicted Close: 6012.37\n",
            "2025-02-21 14:58:54 - Predicted Close: 6011.80\n",
            "2025-02-21 14:59:54 - Predicted Close: 6011.87\n",
            "2025-02-21 15:00:54 - Predicted Close: 6011.69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dLVw25l5GKMp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}